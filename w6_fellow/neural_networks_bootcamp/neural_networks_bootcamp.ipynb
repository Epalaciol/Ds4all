{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks\n",
    "\n",
    "Over the last 10 years, advances in computing power, statistical techniques and algorithmic ingenuity have brought an unprecedented level of accuracy in one particular type of Machine Learning model: the **Neural Network**. Neural networks now allow us to have an extremely high level of precision for all kinds of tasks such as:\n",
    "\n",
    "- Speech Recognition (Siri, Alexa & Google Assistant)\n",
    "- Speech Synthesis (Tacotron)\n",
    "- Text parsing (BERT)\n",
    "- Text Creation (GPT-3)\n",
    "- Object Recognition (YOLO v4, Instagram/TikTok face filters)\n",
    "- Image and Video Creation and Manipulation (PULSE, DeepFakes, StarGAN v2)\n",
    "- All kinds of Classification and Regression (too many examples to count!)\n",
    "\n",
    "These advances have not only been achieved extremely quickly and to an accelerating rate, but they have also reached the hands of everyday consumers in record time, changing the way the world works. Neural Networks are behind Tesla's self-driving cars, your phone's assistant, the recommendation system for Netflix, the Colombian Governments Tax Evaders check, the load balancing in the Electricity grid, the activation of an alarm system and many, many other parts of your everyday life. As the technology develops, the cons of using neural networks are ever-less important, while its pros shine more brightly than before. Soon, it's likely that almost every service in our life will be touched by a neural network in one way or another.\n",
    "\n",
    "## But... what exactly is a Neural Network?\n",
    "\n",
    "A neural network is a type of **Machine Learning Model**. This particular type of model derives its name from how we believe our own neurons work: as a series of simple components, that together can achieve amazing things.\n",
    "\n",
    "### The Neuron\n",
    "\n",
    "To understand a neural network, let's dissect it from the very beginning. First, we need to understand its smallest building block: the neuron. Remember back to the basics of statistics, we learnt how we can model the relationship between one dependent variable $y$ and a set of independent variables $X$. This relationship can be thought of as an equation like $y = f(X)$. The exact form of $f(x)$ is not important, all that matters is that we can estimate a value for $y$ given a set of $X$.\n",
    "\n",
    "A neuron is exactly that! A function which receives a set of inputs and gives an output. The way the neuron does this happens in 2 steps:\n",
    "\n",
    "1. The neuron takes all its inputs, multiplies each of them by a weight exclusive to that input, and adds all of the results together. \n",
    "2. It takes the result of the first step and uses another function, called the _activation function_ of the neuron, to determine what $y$ will be.\n",
    "\n",
    "![image.png](media/image_1.png)\n",
    "\n",
    "While the first step is pretty standardized, there are a myriad of __activation functions__ in use today. The most popular one at the moment is the ReLU (short for Rectified Linear Unit), but there are many more. What most of them have in common is that they output a number between -1 and 1 or between 0 and 1. However, beyond that, there are many differences in these functions. [Here](https://en.wikipedia.org/wiki/Activation_function#Comparison_of_activation_functions) is an incomplete list.\n",
    "\n",
    "\n",
    "That is the short definition of what a neuron is! Just a simple function that takes inputs and outputs a single number. Now, you may be wondering where these inputs come from... That's where the _network_ part of a neural network comes in.\n",
    "\n",
    "### The Network\n",
    "\n",
    "The way neural networks work is by connecting several neurons in a row. You can imagine a neural network as an interconnected series of neurons. Usually, these neurons are arranged in sequential order in what are called __layers__, although not always. The first layer of a neural network is called the __input layer__, the final layer is called the __output layer__, and every layer in between is called a __hidden layer__. The reason they are called hidden will be apparent soon! Here is an example of a very simple neural network with 1 input layer, 1 hidden layer and 1 output layer:\n",
    "\n",
    "![image.png](media/image_3.png)\n",
    "\n",
    "[Image source](https://en.wikipedia.org/wiki/Neural_network)\n",
    "\n",
    "Each of the above circles is a neuron that works like we explained before: \n",
    "1. The neurons in the first layer use our original input $X$ as their set of inputs.\n",
    "2. The neurons assign a weight to each of the inputs.\n",
    "3. The weighted inputs are summed.\n",
    "4. The _activation function_ is run on the sum.\n",
    "5. The neurons output the result of the _activation function_.\n",
    "\n",
    "Note how each neuron has an individual set of weights; every neuron \"interprets\" the set of inputs differently.\n",
    "\n",
    "The second layer neurons, or the first _hidden layer_ neurons, as they are more commonly known, each take the results of the input layer (instead of $X$) and applies the above process again! In this example this happens only once, but you may have as many hidden layers as you want; some large models have thousands of hidden layers! The number of neurons in each layer is the __width__ of the network. Note that each layer may have a different width.\n",
    "\n",
    "The third and final step is done by the output layer: it does the same process again using the output of the final hidden layer, except it uses special kinds of activation functions that allow it to give an output that fits our problem. For example, if we are doing regression, it may use a linear function in order to return a number larger than 1, and for classification it may use a sigmoid function in order to return a probability between 0 and 1. What's important is that the final layer gives us our result: given the set of inputs we gave the input layer, our output layer gives us a corresponding output that hopefully provides a good prediction for our particular problem.\n",
    "\n",
    "Now, how do we know if our result was good or not? This question is deeper than it sounds, and its answer depends on if we are solving a supervised or unsupervised problem. However, we can generally apply a testing strategy used in other Machine Learning model: Run whatever performance metric we want on the result, and see how the model performs in comparison to our true labels (in case of a supervised problem), or how much of a fit to the data we have (for unsupervised problems).\n",
    "\n",
    "#### Correcting course: Backpropagation\n",
    "\n",
    "An important detail is that the initial weights used in the neural network were randomly chosen. The chances that a series of multiplications and sums, where half the factors were randomly determined, will end up in giving us a number close to what we expected or any kind of good performance is very, very low. Impossible, even. So, how does the network work out what the correct weights are to end up with a good performance?\n",
    "\n",
    "The secret is __backpropagation__, or backprop for short. Remember how in calculus class you could find the gradient for a given function by finding its derivative, and you could find an optimal point by seeing where this gradient is zero? Well, backpropagation is something akin to that. In simplified terms, backpropagation is the process of finding the gradient for our neural network's __error functions__ (which is often the same as our performance metric, like RMSE or MAE) for each neuron's respective weight vector. As in calculus class, we go down the slope for each weight vector's gradient until we find a point where the slope is 0 or close to 0. Selecting these weights allows us to create the best performing network that minimize our errors.\n",
    "\n",
    "![image.png](media/image_4.png)\n",
    "\n",
    "This process is called Backpropagation because the process determines the errors after receiving the result from the output layer. Given any input and our current weights, we find an output, calculate its error, and use this error to send information _back_ in the network to each neuron, letting us find its gradient and adjust the weights.\n",
    "\n",
    "Neural networks do this process over and over and over, each time adjusting the weights of each neuron a little so that the error gets lower and lower each time, until we get to a point where the error cannot be reduced any further. This process is called __training__. After we have _trained_ our neural network, we can expect to have a good predictor for our particular problem. Each cycle of learning is called an __epoch__.\n",
    "\n",
    "This process has a fundamental problem: we don't know what each layer is doing. Its final value is obviously important to the final result, but given the large number of connections between each neuron and future weights, the interpretability of what each hidden layer does is almost 0. Hence why they are called _hidden_. The neural network is very much like a _black box_ as we see what's going in and what's coming out, but we don't see the transformation process.\n",
    "\n",
    "If you want to read more on backpropagation, check out this [in-depth guide](https://towardsdatascience.com/understanding-backpropagation-algorithm-7bb3aa2f95fd)\n",
    "\n",
    "## Let's build one!\n",
    "\n",
    "Now, let's put all this talk to practice. The Python library typically used to build a neural network is either Tensorflow or Pytorch. For this bootcamp, we will use Tensorflow in order to classify the Iris dataset, which is a dataset of flowers typically used in a classification problem. We have several flowers, each with different characteristics, and for each of them, we want to predict which of the 3 possible species of _Iris_ flower they are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = datasets.load_iris(return_X_y=True, as_frame=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then have 150 rows, each of which represents a flower. Each flower has 4 features we can use for our prediction, and each flower belongs to 1 of 3 species, represented by the number 0 through 2. The task for our neural network is to train with this dataset and then predict the species of a set of flowers that the neural network has not seen before.\n",
    "\n",
    "![image.png](media/image_5.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's get a train and test set. We'll use 20% of our database (30 flowers) as a test set.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step when using Tensorflow is to initialize the model. The model we saw in previous examples is a \"Sequential\" model, so we will ask Tensorflow to initialize it without any layers at first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we initialize the model.\n",
    "model = tf.keras.models.Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will add our layers to this empty model. We will first add our input layer, then a couple of hidden layers, and finally our output layer. Notice how we chose to use ReLU functions as our activation function, but we could have just as well chosen one of many others. We choose 10 neurons as our width for no particular reason. Usually, a larger model gives better results, but needs more and more diverse data to work well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First our input layer. For this layer, and this layer only, we need to specify the size of our input. For our dataset this means the amount of columns in our X.\n",
    "model.add(tf.keras.layers.Dense(10, activation=tf.nn.relu, input_shape=(X.shape[1],)))\n",
    "# Now some hidden layers\n",
    "model.add(tf.keras.layers.Dense(10, activation=tf.nn.relu))\n",
    "model.add(tf.keras.layers.Dense(10, activation=tf.nn.relu))\n",
    "# Finally, our output layer. Since we have 3 possible classes, we need 3 output neurons. \n",
    "# For a regression problem, we would have only 1. For an image creation network, we would have as many pixels as the image we wanted to create!\n",
    "model.add(tf.keras.layers.Dense(3))\n",
    "# A final layer with several output neurons gives us logits as results. We can do a final pass with a Softmax layer to turn them into percentages.\n",
    "model.add(tf.keras.layers.Softmax())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will decide on an _optimizer_. The optimizer is the algorithm which determines how much we move each weight down the slope we found by backpropagation. There are many possible choices, most of which are based on Stochastic Gradient Descent. [One of the most popular ones right now is \"Adam\"](https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/#:~:text=Adam%20is%20an%20optimization%20algorithm,iterative%20based%20in%20training%20data.&text=The%20algorithm%20is%20called%20Adam.), so that's the one we will use. The word \"stochastic\" means random process. That is why neural networks don't always get the same results even if you train them twice on exactly the same data with the same parameters. Making the process random makes it quicker, but less reliable. SGD and its child algorithms are a large part of the reason why training neural networks is now feasible.\n",
    "\n",
    "The __learning rate__ is the amount of space we expect to move down the slope on each cycle of learning. The larger this number, the more we move, but the less precise the movement.\n",
    "\n",
    "<table>\n",
    "  <tr><td>\n",
    "    <img src=\"media/optims.gif\" width=\"70%\"\n",
    "         alt=\"Optimization algorithms visualized over time in 3D space.\">\n",
    "  </td></tr>\n",
    "  <tr><td align=\"center\">\n",
    "   Optimization algorithms visualized over time in 3D space.<br/>(Source: <a href=\"http://cs231n.github.io/neural-networks-3/\">Stanford class CS231n</a>, MIT License, Image credit: <a href=\"https://twitter.com/alecrad\">Alec Radford</a>)\n",
    "  </td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = tf.keras.optimizers.Adam(learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we compile our final model and train with our data. Notice how we need to define a loss function, but we can also calculate other additional metrics to keep tabs on how our model is doing. The reason we can't use accuracy as our loss function is because the loss function _must_ be differentiable in order for backpropagation to work.\n",
    "\n",
    "We also define a batch size: This is the amount of example we feed into our model at a time. You usually want this to be a power of 2, due to the way GPUs process data. 32 is a good starting point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "    optimizer=optim,metrics=['accuracy'])\n",
    "history = model.fit(X_train.values, y_train.values,\n",
    "        validation_data=(X_test.values, y_test.values),\n",
    "        epochs = 20,\n",
    "        batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model is now trained! We can see both the performance it had vs the data it was training on (`loss` and `accuracy`), and its performance vs the test data as it trained (`val_loss` and `val_accuracy`). This history an be easily saved since the `.fit()` method returns the results for each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot history\n",
    "plt.plot(history.history['loss'], label='Sparse Categorical Crossentropy (training data)')\n",
    "plt.plot(history.history['val_loss'], label='Sparse Categorical Crossentropy (validation data)')\n",
    "plt.title('Training History')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch #')\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can use our model to make predictions! This is as easy as calling the `.predict()` method, with a list of examples to get a prediction out of."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's get the prediction for the first flower in the test set\n",
    "model.predict(X_test[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we used a Softmax function in our model, the predictions return a list of probabilities which add up to one. Each probability corresponds to how confident our model is that the flower corresponds to a particular species. We can grab the highest of these predictions as our actual species, and compare it to the real one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test)\n",
    "for idx, prediction in enumerate(predictions):\n",
    "    print('We predict: '+str(np.argmax(prediction))+'. Real Species was: '+str(y_test.iloc[idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now built a simple neural network from scratch!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Pros and Cons\n",
    "\n",
    "Neural networks are amazing, but they are not perfect. As with everything in Data Science, they have good things and bad things about them. It's tempting to use a neural network for everything you see as they can actually solve almost any problem you throw at them. But it is important not to give into this tunnel vision and first explore alternatives before using a neural network.\n",
    "\n",
    "The mains Cons of neural networks are:\n",
    "\n",
    "1. **Computationally expensive**: Neural networks are _very_ computationally expensive to train when compared with other machine learning models. Training NNs efficiently today is only possible because of the combination of backpropagation with the advent of modern GPUs (graphic cards), which are processors that are extremely good at solving the kind of computations that backpropagation requires. This means that you NEED a GPU to train a Neural Network efficiently, and they are not cheap. Even with a GPU in hand, training a Neural Network typically takes longer than training other machine learning models, and with large models, this time constraint becomes very real (it could take months to train a single version of your network).\n",
    "2. **Interpretability**: Neural networks are, by definition, complex. The fact that hidden layers are _hidden_ makes interpretability of the neural network very hard. It's basically impossible to find out why a neural network is giving us the results it is after its been trained, even though the results may be great. This may be irksome to those using the network: why is it telling me this cat is a dog? This and other such questions (but this question above all others) are the bane of Neural Network developers and users since their invention.\n",
    "3. **Reliability**: Even though we may be getting amazing results 99.999% of the time, there may come a time when the network decides that it should turn your car right instead of left out of the blue, even if its just 1 time out of a million, just because we gave it the _exact_ wrong set of inputs. And when it does, we won't truly know why it behaved that way since it is not interpretable. This is a huge problem that forces neural networks in charge of sensitive systems to undergo additional checks. Other machine learning models are much more or even 100% reliable.\n",
    "4. **Finicky**: The lack of interpretability of neural networks also causes a large problem when developing one. The process of finding the optimal number of neurons and layers to use, selecting an activation function, what set of features to feed it, and any of the other _hyperparameters_ needed to configure it is little more than educated trial and error. There is currently no mathematical formula to calculate any of these parameters, and the best method is using a random search over a set of possible values for each parameter, try out that combination, and see if it was better than the previous random combination. This means that _a lot_ of time is spent finding the perfect combination for your data.\n",
    "\n",
    "Now the pros:\n",
    "\n",
    "1. **Flexible**: Neural networks are _extremely_ flexible. Nowadays they can be used to solve mostly any problem you can think of. This doesn't mean they are _better_ than other models for the same problem, but there is no other Machine Learning model that can be used to solve such a diverse set of problems.\n",
    "2. **High Dimensionality**: Neural networks excel at problems with high dimensionality. That is, problems where we use a very large number of inputs in order to get an output. This is why Neural Networks are so useful for computer vision and voice recognition problems, as there is an extremely large number of variables to be considered in order to make a decision. This can be taken to extreme levels: the GPT-3 language model has over 15 billion parameters.\n",
    "3. **Speed**: Neural networks, once trained, are actually quite efficient to use. This means that neural network models can be used for processes that require large quantities of predictions in small amounts of time.\n",
    "4. **Scalable**: Neural networks have shown that they are limited only by 2 factors: computation power and quantity of data. In general, a neural network will increase its performance as long as you either feed it more data or increase the depth and width of your model. Other optimization can help but are less potent. Competing machine learning models don't necessarily scale past a certain point, no matter how much computational power or data you throw at it.\n",
    "\n",
    "Given these sets of pros and cons, the general recommendation is to _not_ use a neural network as your first model of choice, no matter how tempting. First, use other simpler models, and see their results. If they are good enough, there is no need to waste the time needed to train and optimize a neural network. But if you see that more traditional models are not cutting it, by all means unleash the power of the neural network on your data. It's very likely that with the right optimizations, enough data, and a ton of computing power, you will end up with a very capable model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attribution\n",
    "\n",
    "\"Artificial neural network\", Knowino, Creative Commons Attribution–ShareAlike 3.0 Unported, https://www.tau.ac.il/~tsirel/dump/Static/knowino.org/wiki/Artificial_neural_network.html\n",
    "\n",
    "\n",
    "\"Simplified view of a feedforward artificial neural network\", Oct. 28, 2008, Wiso, Public Domain, https://commons.wikimedia.org/wiki/File:Neural_network_example.svg\n",
    "\n",
    "\"Gradient descent method\", Feb 21, 2015, Роман Сузи, Creative Commons Attribution-Share Alike 3.0 Unported, 2.5 Generic, 2.0 Generic and 1.0 Generic license, https://commons.wikimedia.org/wiki/File:Gradient_descent_method.png\n",
    "\n",
    "\"Iris versicolor 3\", July 9, 2005, Dlanglois, Creative Commons Attribution-Share Alike 3.0 Unported license, https://commons.wikimedia.org/wiki/File:Iris_versicolor_3.jpg\n",
    "\n",
    "\"Optimization algorithms visualized over time in 3D space\", Stanford class CS231n (Alec Radford), MIT License, https://cs231n.github.io/neural-networks-3/"
   ]
  }
 ],
 "metadata": {
  "c1_recart": "7.0.0-57c20131aabc1dc2a8c675852d80a7da"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
