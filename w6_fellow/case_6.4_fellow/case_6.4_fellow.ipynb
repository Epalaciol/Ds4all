{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How much should properties be worth in Milwaukee, Wisconsin? (Part II)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "3_min"
    ]
   },
   "source": [
    "## Goals\n",
    "\n",
    "In the last few cases, we have seen several techniques used to diagnose the applicability of linear regression models. We have learned how to use indicators such as AIC to compare and detect overcomplicated models, how a proper study of the residuals can help to fit our models better, and how to extend the applicability of linear models with the use of variable transformations. The goal of this case is to learn a couple of tools that will help us further understand the applicability of our models better:\n",
    "\n",
    "1. The use of test/training sets to evaluate the performance of our models\n",
    "2. Detection of **multicolinearity** to improve numerical stability and increase the robustness of the predictive model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "5_min"
    ]
   },
   "source": [
    "## Introduction\n",
    "\n",
    "**Business Context.** You are the same property developer from the previous case, with the same goal. Although the previous model you built was a good start, it did not incorporate all the variables you wished to include, and you are skeptical of how well it might work on data that it was not trained on; i.e. **out-of-sample data**.\n",
    "\n",
    "**Business Problem.** Your task is to **improve your model to predict property prices in the city of Milwaukee, Wisconsin**.\n",
    "\n",
    "**Analytical Context.** Again, the dataset consists of property sales (commercial and residential) in Milwaukee, Wisconsin from 2002 to 2018. focused on diagnosing and fixing possible issues arising in linear regression. In the previous case, We learned how to visually analyze residuals and to detect outliers and heteroscedasticity. We showed that variable transformation can improve some of these issues, while explicit removal of outliers *explainable by external factors* could improve things further.\n",
    "\n",
    "This case is structured as follows: you will 1) learn how categorical variables are actually handled in regression models; 2) dive into the predictive ability of the model and learn how to assess and improve it; and finally 3) look at **multicollinearity**, an issue that arises when fitting regression models with highly correlated or too many predictors and how to deal with it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "5_min"
    ]
   },
   "source": [
    "## Preparing our data\n",
    "\n",
    "We will pick up were we left things off at the end of the last case. We will load the same packages and data, and fit the same model. The one difference is that we will remove from the dataset all properties with a sale price\n",
    "below $2,000, which may not correspond to real market prices, per our analysis of residuals in the last exercise of the previous case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load relevant packages\n",
    "import pandas                  as pd\n",
    "import numpy                   as np\n",
    "import matplotlib.pyplot       as plt\n",
    "import seaborn                 as sns\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.api         as sm\n",
    "import scipy\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "register_matplotlib_converters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"2002-2018-property-sales-data.csv\",\n",
    "    dtype = { # indicate categorical variables\n",
    "        \"PropType\": \"category\",\n",
    "        \"District\": \"category\",\n",
    "        \"Extwall\": \"category\",\n",
    "        \"Nbhd\": \"category\",\n",
    "        \"Style\": \"category\",\n",
    "    },\n",
    "    parse_dates=[\"Sale_date\"], # the Sale_date column is parsed as a date\n",
    ")\n",
    "def remove_unused_categories(data):\n",
    "    \"\"\" The `remove_unused_categories` method in pandas\n",
    "        removes categories from a Series if there are no\n",
    "        elements of that category.\n",
    "        \n",
    "        This function is a convenience function that removes\n",
    "        unused categories for all categorical columns\n",
    "        of a data frame.\n",
    "        \n",
    "        The reason this is useful is that when we\n",
    "        fit a linear regression, `statsmodels` will\n",
    "        create a coefficient for every category in a column,\n",
    "        and so unused categories pollute the results.\n",
    "    \"\"\"\n",
    "    for cname in data:\n",
    "        col = data[cname]\n",
    "        if pd.api.types.is_categorical_dtype(col):\n",
    "            data[cname] = col.cat.remove_unused_categories()\n",
    "    return data\n",
    "\n",
    "clean = np.where(\n",
    "    (data[\"Sale_price\"] > 2000) & # this is the only change!\n",
    "    (data[\"Year_Built\"] > 1800) &\n",
    "    (data[\"Fin_sqft\"] > 0) & # must have non-zero finished square feet\n",
    "    (data[\"Lotsize\"] > 0)  & # must have non-zero lot size\n",
    "    (data[\"PropType\"] == \"Residential\")\n",
    "    )\n",
    "data_clean = data.iloc[clean].copy()\n",
    "remove_unused_categories(data_clean).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "25_min"
    ]
   },
   "source": [
    "## Training vs. test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we discussed in the introduction, one of our chief concerns is whether or not the model we have built works just as well on out-of-sample data as it does on in-sample data. This is a *very* common problem in model-building, as is known as **overfitting**. You will learn more about overfitting in future cases, but here we will discuss a simple method for mitigating it.\n",
    "\n",
    "The idea is to randomly split the data into a training set and a test set. The **training set** is the one on which we train and fit our multiple linear regression model. We then run our fitted model on the **test set** and compared its predictions against the actual test set response variable data to evaluate its performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "10_min"
    ]
   },
   "source": [
    "### Exercise 1:\n",
    "\n",
    "Write code to split the data into training and test sets (an 80-20 split is a good starting point). Fit a linear regression model on the training set, with the logarithm of `Sale_price` as the response variable and `District`, `Units`, and the logarithm of `Fin_sqft` as the predictor variables.'\n",
    "\n",
    "**Note:** In order to make the results reproducible, write ```np.random.seed(135568109)``` before your code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "ans_st"
    ]
   },
   "source": [
    "**Answer.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "10_min"
    ]
   },
   "source": [
    "### Categorical features under the hood\n",
    "\n",
    "You may have noticed that there are over a dozen coefficients for `District` above. This is because `District` is a categorical variable, and for categorical features, one coefficient is obtained for all but one of the categories. If there are only two categories (e.g. gender), this is intuitive: the feature is converted into a column of zeros and ones before feeding it into the regression, where it is treated like a regular numerical variable.\n",
    "\n",
    "When there are more than two categories, one category is designated the “reference” or “baseline” category, and “dummy” columns of ones and zeros are created for all other categories. Let's take a dummy example with three categories and five rows:\n",
    "\n",
    "| Category        |\n",
    "|--------------|\n",
    "| A            |\n",
    "| B            |\n",
    "| C            |\n",
    "| C            |\n",
    "| A            |\n",
    "\n",
    "Before the linear regression is fitted, the `Category` column is transformed into **two** “dummy” columns (a column for baseline category is not added).\n",
    "\n",
    "The first column is 1 if the district is `B`, and 0 otherwise, whereas the second column is 1 if the district is `C`, and 0 otherwise.  We get:\n",
    "\n",
    "| Category_B            | Category_C           |\n",
    "|--------------|-------------|\n",
    "| 0            | 0           |\n",
    "| 1            | 0           |\n",
    "| 0            | 1           |\n",
    "| 0            | 1           |\n",
    "| 0            | 0           |\n",
    "\n",
    "The dummy columns are fed into the linear regression and treated like regular numerical variables. This technique is called **one-hot encoding** and can be done manually with the pandas function `pd.get_dummies()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dummy=pd.DataFrame({'Category':['A','B','C','C','A']})\n",
    "pd.get_dummies(df_dummy,columns=['Category'], drop_first=True) #Use drop_first to drop the first category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like in the binary case, the choice of baseline changes the coefficients and their interpretation; the `District[T.3]` coefficient of 1.0236 should be interpreted as the difference in predicted outcomes between districts 3 and 1. But the baseline choice does not affect predictions and model performance, which is why most software will arbitrarily pick one category as the baseline without warning (often the first alphabetically)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "15_min"
    ]
   },
   "source": [
    "### Exercise 2:\n",
    "\n",
    "#### 2.1\n",
    "\n",
    "Add `Style` to our last regression model. Do this in two ways: first by transforming the `Style` variable using `pd.get_dummies()` and then without the transformation. Verify that you obtain the same results. According to this model, which property style is the most desirable?\n",
    "\n",
    "**Hint:** The `Style` variable contains names, like `Residence O/S`, which may be problematic when writing `smf.ols` formulas (see the [Pasty Syntax](https://patsy.readthedocs.io/en/latest/index.html)). You can use the `pasty` quote `Q()` syntax on variable names to avoid this issue:\n",
    "```\n",
    "formula=\"np.log(Sale_price) ~Q(\"Residence O/S\")+...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "ans_st"
    ]
   },
   "source": [
    "**Answer.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2\n",
    "\n",
    "In the model below, we have *not* removed the baseline category after using ```get_dummies()```. Is this model correct? Why or why not? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dummy2=pd.get_dummies(train,columns=['Style'],prefix='',prefix_sep='')\n",
    "formula1=\"np.log(Sale_price) ~ \"\n",
    "for style in Styles:\n",
    "    formula1+='Q(\"'+style+'\")+'\n",
    "formula1=formula1+'District + Units+ np.log(Fin_sqft)'\n",
    "\n",
    "#Create the model\n",
    "model_style_2 = smf.ols(formula = formula1, \n",
    "                 data = train_dummy2).fit()\n",
    "model_style_2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "ans_st"
    ]
   },
   "source": [
    "**Answer.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "30_min"
    ]
   },
   "source": [
    "## Evaluating predictive performance\n",
    "\n",
    "Recall that for any given data point, the residual is the difference between our model's prediction for that point and the actual value of that point. The **mean absolute error (MAE)** is a metric which summarizes the model's holistic performance on the entire dataset. The MAE is calculated by taking the absolute value of each residual, then taking the mean of all of those absolute values. In essence, the MAE describes the typical magnitude of the residuals; the lower the MAE, the better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MAE(prediction,true_values):\n",
    "    return np.mean(                                                      # Mean\n",
    "                np.abs(                                                   # Absolute\n",
    "                        prediction-true_values                            # Error\n",
    "                    )\n",
    "                )\n",
    "\n",
    "print(\"MAE between model_log and log of true Sale price:\",MAE(model_log.predict(test) ,np.log(test.Sale_price)))\n",
    "print(\"MAE between exp(model_log) and true Sale price:\",MAE(np.exp(model_log.predict(test)) ,test.Sale_price))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another very popular choice for evaluating regression is the **root mean squared error (RMSE)**. This is computed by taking the square of each residual, averaging them, and then taking the square root. Below is the implementation in python, which is possibly easier to understand than the above explanation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RMSE(prediction,true_values):\n",
    "    \n",
    "    return np.sqrt(                                                          # Root\n",
    "            np.mean(                                                      # Mean\n",
    "                np.square(                                                # Squared\n",
    "                         prediction-true_values                           # Error\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "        \n",
    "print(\"RMSE between model_log and log of true Sale price:\", RMSE(model_log.predict(test) ,np.log(test.Sale_price)))\n",
    "print(\"RMSE between exp(model_log) and true Sale price:\",RMSE(np.exp(model_log.predict(test)) ,test.Sale_price))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, there is the **mean absolute percentage error**. This takes the absolute value of each residual and divides it by the actual value of that point to obtain a percentage, then averaging across all percentages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MAPE(prediction,true_value):\n",
    "    return np.mean(                                           # Mean\n",
    "        np.abs(                                               # Absolute\n",
    "               (prediction-true_value)/true_value             # Error\n",
    "            )*100                                            # Percentage\n",
    "    )\n",
    "\n",
    "print(\"MAPE between model_log and log of true Sale price:\", MAPE(model_log.predict(test) ,np.log(test.Sale_price)))\n",
    "print(\"MAPE between exp(model_log) and true Sale price:\", MAPE(np.exp(model_log.predict(test)) ,test.Sale_price))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "5_min"
    ]
   },
   "source": [
    "### Question:\n",
    "\n",
    "Why would we not just use MAE instead of having to deal with three different metrics?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although RMSE may seem needlessly complicated compared to MAE, it is much more commonly used. The reason is that the RMSE metric is the same as the one being minimized on the training data by a standard linear regression model (also called an **ordinary least squares (OLS)** regression). This makes it a “natural” choice using the same metric to evaluate the out-of-sample (test) performance.\n",
    "\n",
    "Furthermore, the RMSE puts much more weight on outliers, since the errors are squared before being averaged. In cases where outliers are especially bad and need to be punished, the RMSE is a better choice.\n",
    "\n",
    "The MAPE has a nice interpretation in that we can say that a model's predictions are on average wrong by a certain percentage. For example, our house price model could be on average off by 30%. Such a model is often described as being \"70% accurate\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For those who are mathematically inclined, the equations below define the three error metrics above:\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "    MAE &= \\frac{1}{n} \\sum_{i=1}^n \\left| y_i - \\hat{y}_i \\right| \\\\\n",
    "    RMSE &= \\sqrt{ \\frac{1}{n} \\sum_{i=1}^n ( y_i - \\hat{y}_i )^2 } \\\\\n",
    "    MAPE &= 100\\% \\frac{1}{n} \\sum_{i=1}^n \\left|\\frac{ y_i - \\hat{y}_i}{y_i} \\right|\n",
    "\\end{eqnarray*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "15_min"
    ]
   },
   "source": [
    "### Exercise 3:\n",
    "\n",
    "#### 3.1 \n",
    "\n",
    "Compute the MAE, RMSE and MAPE for the linear model \n",
    "``` \n",
    "\"Sale_price ~ District + Units+ Fin_sqft\"\n",
    "```\n",
    "How does this model compares to ```model_log```?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "ans_st"
    ]
   },
   "source": [
    "**Answer.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2\n",
    "\n",
    "Let us illustrate the differences between MAE, RMSE and MAPE with an example. In the following data, our test sample has been contaminated with a single outlier. Compare the MAE, RMSE and MAPE of the models:\n",
    "```\n",
    "model1: y~x\n",
    "model2: y~(7.8182)x-24.3557\n",
    "```\n",
    "Use the test data ```x_test, y_test``` provided below. What do you see? Which model is better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(135568109)\n",
    "error=np.random.normal(0,0.1,100)\n",
    "x_data=np.arange(0,10,0.1)\n",
    "y_data=x_data+error\n",
    "\n",
    "#Split the data into training and test data:\n",
    "train_ex=np.random.randint(0,99,80)\n",
    "x_train=x_data[train_ex]\n",
    "y_train=y_data[train_ex]\n",
    "\n",
    "test_ex=list(set(range(100))-set(train_ex))\n",
    "x_test=x_data[test_ex]\n",
    "y_test=y_data[test_ex]\n",
    "\n",
    "#oops it got contaminated\n",
    "x_test=np.append(x_test,11)\n",
    "y_test=np.append(y_test,1000)\n",
    "\n",
    "\n",
    "fig, axes=plt.subplots(nrows=1, ncols=2,figsize=(16,8))\n",
    "sns.regplot(x=x_train, y=y_train,ax=axes[0])\n",
    "axes[0].set_title('Training Data')\n",
    "\n",
    "sns.regplot(x=x_test, y=y_test,ax=axes[1])\n",
    "axes[1].set_title('Test Data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "ans_st"
    ]
   },
   "source": [
    "**Answer.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "30_min"
    ]
   },
   "source": [
    "## Adding additional predictors to improve our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider now `model_feat` described below. This model adds features for the number of stories and bedrooms, the year built, neighborhood (instead of simply district), type of external wall, number of full and half baths, lot size and sale year. This last predictor is an example of **feature engineering** – remember we are not limited to the predictors as they are given to us in the raw data. Often, it is beneficial to transform and combine predictors, as we did here by extracting the year from the sale date and treating it as a categorical variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "30_min"
    ]
   },
   "source": [
    "### Exercise 4:\n",
    "\n",
    "#### 4.1\n",
    "\n",
    "Fit a linear regression of the logarithm of the sale price with the following features:\n",
    "\n",
    "1. number of stories\n",
    "2. number of bedrooms\n",
    "3. number of units in the property\n",
    "4. neighborhood code\n",
    "5. style\n",
    "6. type of external wall\n",
    "7. number of full baths\n",
    "8. number of half baths\n",
    "9. the logarithm of square footage\n",
    "10. the logarithm of the lot size\n",
    "11. year the property was *built* as a *numerical* variable\n",
    "12. year the property was *sold* as a *categorical* variable\n",
    "\n",
    "Give the resulting fitted model the variable name `model_feat`.\n",
    "\n",
    "**Hint:**\n",
    "\n",
    "1) write a function to extract the year from the sale date – `train.Sale_date.iloc[0].year` gives the year of the first sale; \n",
    "\n",
    "2) [use `C()` around a term in the formula to make the term categorical](https://patsy.readthedocs.io/en/latest/categorical-coding.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "ans_st"
    ]
   },
   "source": [
    "**Answer.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2\n",
    "\n",
    "How should the coefficient `Extwall[T.Brick]` of 0.0236 be interpreted? Select the appropriate choice below:\n",
    "\n",
    "\"A property with brick external walls is predicted to have a log price that is...\"\n",
    "\n",
    "(a) 0.0236 higher than a property with aluminum / vinyl external walls that is otherwise identical\n",
    "\n",
    "(b) increased by 0.0236, all else being equal\n",
    "\n",
    "(c) 0.0236 higher than the average property in the training dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "ans_st"
    ]
   },
   "source": [
    "**Answer.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3\n",
    "\n",
    "Compute the mean absolute error for `model_feat`, and compare it to `model_log`. Does `model_feat` perform better or worse? What about the $R^2$ and AIC of these two models?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "ans_st"
    ]
   },
   "source": [
    "**Answer.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4\n",
    "What story does this model tell us about the Milwaukee housing market during the last 15 years?\n",
    "\n",
    "(a) The prices increased until 2007, when there was a housing crash. Since then, prices have gone back up to their pre-crash level\n",
    "\n",
    "(b) Overall, housing prices steadily increased over time\n",
    "\n",
    "(c) The prices increased until 2007, then subsequently went down\n",
    "\n",
    "(d) The prices increased until 2007, when there was a housing crash. Since then, prices have slowly risen, but they have not yet fully recovered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "ans_st"
    ]
   },
   "source": [
    "**Answer.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.5\n",
    "\n",
    "A 2-story townhouse is on the market for $150,000 with the following characteristics:\n",
    "\n",
    "1. Neighborhood 4320 in District 11\n",
    "2. 1200 finished square feet\n",
    "3. 7200 square feet lot size\n",
    "4. 3 bedrooms\n",
    "5. Vinyl external wall\n",
    "6. built in 1987\n",
    "7. 1 full bath and 1 half-bath\n",
    "8. 1 unit\n",
    "\n",
    "Is this a good price according to our model? What assumptions do you have to make to answer this question?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "ans_st"
    ]
   },
   "source": [
    "**Answer.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "20_min"
    ]
   },
   "source": [
    "## Collinearity and standard errors\n",
    "\n",
    "The model in the previous section seems to be a good step towards our main goal. However, from the last few cases we have learned that adding a large dump of new features to a model and then leaving it to its own devices is not a prudent strategy, because:\n",
    "\n",
    "1. We do not want to include features that add little predictive power to our model. Otherwise, we may end up with an overcomplicated model. Remember that we generally want a model that achieves a good level of predictiveness without sacrificing simplicity\n",
    "2. Adding too many features increases the danger of *overfitting*, thus decreasing the potential applicability of the model\n",
    "\n",
    "So far, we have learned a couple of methods to alleviate these issues:\n",
    "\n",
    "1. We can use indicators such as the ```AIC``` to detect if we are adding variables with little predictive power\n",
    "2. We can divide our sample data into `Train/Test` sets and use the `Test` set, along with some measure like `RMSE`, to evaluate the applicability of our model\n",
    "\n",
    "However, there exists a third potential issue whenever we add too many features to our models: \n",
    "\n",
    "3. Linear regression can actually be **destabilized** when there are too many predictors that do not add value\n",
    "\n",
    "This means that small changes in the training data fed into the model results in large changes in the fitted coefficients; overfitting is a common consequence of this phenomenon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "5_min"
    ]
   },
   "source": [
    "### Question:\n",
    "\n",
    "What do you think causes the above to happen?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As one adds more predictors, they begin to contribute redundant information (which explains why every additional predictor contributes zero marginal information). Mathematically, this means that new predictors can often be expressed to a good extent as linear combinations (weighted sums) of some of the other predictors. When this happens, a model is said to exhibit **collinearity** or **multicollinearity**.\n",
    "\n",
    "Since predictors become mostly redundant, this means that nearly identical predictions can be obtained by vastly different coefficient values, by \"trading off\" between the coefficients of the predictors which exhibit collinearity.\n",
    "Consequently, the model has no way to decide from the data alone which of these possible models with identical predictions to choose, which translates into inflated standard errors and a high risk of overfitting.\n",
    "\n",
    "To demonstrate this effect, we will introduce an engineered feature: `log(Fin_sqft/Lotsize + 0.00001)`.\n",
    "The small constant is there to avoid making the new feature *perfectly* collinear with log(`Fin_sqft`) and log(`Lotsize`) (which would create a \"divide-by-zero\" error in the background regression function code):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base model without engineered collinear feature\n",
    "model_finlot = smf.ols(formula = \"np.log(Sale_price) ~ \"\n",
    "                           \"Stories + Year_Built\"\n",
    "                           \"+ np.log(Fin_sqft)\"\n",
    "                           \"+ np.log(Lotsize)\"\n",
    "                           \"+ Style\"\n",
    "                           \"+ Extwall  + Units + Bdrms\"\n",
    "                           \"+ Fbath + Hbath + Nbhd\"\n",
    "                           \"+ C(year_from_date(Sale_date))\", \n",
    "                 data = train).fit()\n",
    "model_finlot.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_coll = smf.ols(formula = \"np.log(Sale_price) ~ \"\n",
    "                           \"Stories + Year_Built\"\n",
    "                           \"+ np.log(Fin_sqft)\"\n",
    "                           \"+ np.log(Lotsize)\"\n",
    "                           \"+ np.log(Fin_sqft/Lotsize + 0.0001)\"\n",
    "                           \"+ Style\"\n",
    "                           \"+ Extwall  + Units + Bdrms\"\n",
    "                           \"+ Fbath + Hbath\"\n",
    "                           \"+ Nbhd\"\n",
    "                           \"+ C(year_from_date(Sale_date))\", \n",
    "                 data = train).fit()\n",
    "model_coll.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "5_min"
    ]
   },
   "source": [
    "### Exercise 5:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1\n",
    "\n",
    "What happened to the coefficients and standard errors of `log(Fin_sqft)` and `log(Lotsize)` as the engineered feature was introduced?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "ans_st"
    ]
   },
   "source": [
    "**Answer.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2: \n",
    "\n",
    "What happens when you change the 0.0001 constant (say, to 0.00001 or 0.001) in the engineered feature?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "ans_st"
    ]
   },
   "source": [
    "**Answer.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To detect and investigate collinearities, it is useful to look at correlations between predictors within a model, shown below. Notice the high correlations (positive *or* negative) between the engineered feature `np.log(Fin_sqft / Lotsize + 0.0001)` and `np.log(Fin_sqft)`/`np.log(Lotsize)`. Note also the other high correlations with bathrooms and number of stories in the matrix. These can destabilize the model and make it difficult to make meaningful statements about the incremental value of adding extra bathrooms and stories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = model_coll.model.data.exog # the design matrix X containing all\n",
    "                               # predictors including dummy variables\n",
    "                               # for categorical variables\n",
    "param_names = model_coll.model.data.param_names # parameter names\n",
    "M_df = pd.DataFrame(M, columns=param_names) # as a pandas data frame\n",
    "# subset of predictors to plot:\n",
    "M_sub = M_df[['np.log(Fin_sqft)','np.log(Lotsize)',\n",
    "              'np.log(Fin_sqft / Lotsize + 0.0001)',\n",
    "              'Bdrms','Fbath','Hbath','Stories']]\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.heatmap(M_sub.corr(), cmap=\"RdYlBu\", \n",
    "    annot=True, square=True,\n",
    "    vmin=-0.8, vmax=0.8, fmt=\"+.1f\")\n",
    "plt.title(\"Correlations between predictors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This illustrates the danger of just throwing additional features into a model. Not only is it redundant, but it can also lead to highly unstable and nonsensical values for the coefficients, which can lead to overfitting in production systems. One way of dealing with this is what we discussed in previous cases – using the AIC criterion to iteratively evaluate each new feature and determine if it is really adding enough incremental value on top of a penalty for the additional complexity. Another is to examine correlation matrices like the one we created above to identify any values close to -1 or 1. In future cases on **cross-validation** and **regularization**, you will learn additional ways to deal with overly complex and potentially overfit models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "5_min"
    ]
   },
   "source": [
    "## Conclusions\n",
    "\n",
    "In this case, we had a closer look at categorical predictors. We then focused on predictions, and discussed a few different metrics for the predictive power of a regression model. Each had their own advantages and disadvantages depending on what the business problem at hand needed to prioritize.\n",
    "\n",
    "To improve the predictive power of our model, we added additional predictors. We learned how to **engineer features** to capture information that was not directly available as a column of the data set. However, we saw how adding so many features at once could lead to **collinearity** and therefore destabilize a model, making coefficients difficult to interpret and increasing the chance of overfit predictions in production."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "5_min"
    ]
   },
   "source": [
    "## Takeaways\n",
    "\n",
    "In this case, we have expanded our toolbox for the diagnosis of linear regression models. We have learned:\n",
    "    \n",
    "1. How the use of training/testing sets, along with measures such as MAE, MAPE and RMSE can help in evaluating the predictive power of linear models. We note that these tools are not only applicable to linear models, but also work with any other type of model and we will continue using them in future cases. \n",
    "2. How categorical variables are encoded in linear regression models.\n",
    "3. That more complex models can provide better predictions, but this can destabilize performance. In many such situations, complexity creates multicollinearity between variables which actually makes our models highly unstable and prone to overfitting. \n",
    "\n",
    "Remember, much of the business world is about applying trained models on future, currently unknown data. Overfitting can lead to serious negative impacts on the bottom line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attribution\n",
    "\n",
    "\"Property sales data, 2002-2018 Master File\", April 29, 2019, Milwaukee OpenData, Creative Commons Attribution, https://data.milwaukee.gov/dataset/property-sales-data/resource/f083631f-e34e-4ad6-aba1-d6d7dd265170\n"
   ]
  }
 ],
 "metadata": {
  "c1_recart": "6.10.0-57c20131aabc1dc2a8c675852d80a7da"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
